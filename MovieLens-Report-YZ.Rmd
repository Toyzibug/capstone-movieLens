---
title: "MovieLens Report YZ"
author: "Yaping Zhang"
date: "22 6 2020"
output: html_document
---


1.Introduction

The project is to create a movie recommendation system using machine learning method based on the MovieLens 10M dataset. Through different machine learning approaches, a model will be trained and optimized to predict the rating of movies, based on which, the movies are recommended to users. The dataset is provided by GroupLens research lab containing numerous movies with ratings from various users. 
In the report the dataset will be first described, furthermore, the goal and key steps will be determined. After that, we will introduce the method of the project, which are the detailed steps to reach the goal. Here we will first clean the dataset and then explore and visualise it to find out the insights for building up the model in next step. We look for the influence factors for ratings in the exploration. Based on the findings, machine learning algorithms with influencing factors(parameters) will be constructed. Based on the model we make the prediction of ratings on test dataset and then evaluate the result using the residual mean squared error (RMSE). Different models might be used until its RMSE reaches the goal. In the end of the report, a conclusion including summary, limitation and future work of the project will be explained.


1.1.Dataset description

The dataset is downloaded from the internet and wrangled into a data frame which then is separated randomly into a training set and a test set. We will explain it in detail:

1.	The dataset is first pulled from the following link:
http://files.grouplens.org/datasets/movielens/ml-10m.zip

2.	Then it is cleaned and converted to a data frame containing 10000054 observations(ratings) of 6 variables(columns) as the following:

```{r, message= FALSE, warning=FALSE,echo=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(lubridate)
library(stringr)
library(dplyr)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
movielens
movielens%>%as.tibble()
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

#Dataset description -- dataset in tibble format
head(edx)

```
Here each row(observation) represents a rating given by the userId to the movieId. Important to notice, not every movie gets rating from every user. Therefore the number of ratings varies. 

3.   The dataset is then randomly separated into two sets called edx and validation. Edx consists of 90% of the movieLens dataset and is only used for training and tuning a model with tests. On the other side, validation dataset, possessing the remaining 10%, is only used to test the model with the final RMSE. Additional to that, the movies only existing in validation are filtered out and added into the edx, to make sure the users and movies included in the test set(validation) exist in the training set(edx). Here we see the dimension of the two datasets:
```{r dimension, echo=TRUE}
##dimention of the training and testing data
dim(edx)
dim(validation)
```


1.2.Goal of the project and key Steps

The goal of the project is to create an optimized machine learning model with RMSE value less than 0.8649. The RMSE is defined as the following:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$
with:	
u, i: user, movie 

ŷu,i, yu,i: predicted rating, the true rating ( from user u of movie i)

N: number of rating 

The key steps of the procedure are the followings:
1.	cleaning the entire data. From the chapter 1.1, we see some columns are not presenting the unique or clear information, eg. Title and coming out year of the movie are included in the same column. Also, the timestamp of the rating is still in the epoch time format.
2.	Exploring the data. This is an essential step of the whole project. Through this step we find out the infulnce factors to support the next step.
3.	Modelling. Based on the last step we will build up two different models only using edx. One will be more complex than the other.
4.	Resulting. We will make the prediction with the models and evaluate the RMSE value using edx test subset. In the end we will test the model with better RMSE value on the validation for the final RMSE.


2.Methods

In this chapter the key steps mentioned above will be explained in detail. In the end of the chapter, the models will be built up and ready for the resulting procedure.

2.1.Data cleaning

Before starting to explore the data, data cleaning can be done to optimize the visualization and analysis. 
First of all, we convert the “timestamp” to “date” using function as.date(). After that, extract coming out year information from the variable “title” and add it as a new column “year”. For this step I tried with several method. There is function such as str_split_fixed(), but it takes quite longer operating time. As a result I used functions str_extract() and str_replace(). The advantage of them is you can be sure of extraction of the correct information and it takes much less time. The dataset looks like the following after the cleaning:
```{r data cleaning, echo=FALSE, Message=FALSE}
# convert timestamp to date type
edx<-edx%>%mutate(date=as.Date(as.POSIXct(timestamp, origin="1970-01-01")))%>%select(-timestamp)

#extract the coming out year to a new column
year<-str_extract(edx$title,"\\(\\d{4}\\)")
year<-str_replace(year,"\\(","")
year<-str_replace(year,"\\)","")
edx<-edx%>%mutate(year=year)
##delete the year info from column "title"
title<-str_replace(edx$title,"\\(\\d{4}\\)", "")
edx<-edx%>%select(-title)%>%mutate(title=title)
as.tibble(edx)
```
The same is processed with dataset validation. Now the data is suitable to be explored and visualized.

2.2.Data exploration and visualization

In this procedure, the main features of the dataset will be explored, in order to find out patterns for next step to build up the model. First we will have a look at the distribution of rating. Furthermore, we inspect the movie and user effects regarding number of ratings.
From the summary of “rating”, we can find out the mean value and range of the ratings:
```{r,echo=TRUE}
summary(edx$rating)
mu<-mean(edx$rating)
```
Here we see the mean rating µ is 3.512, while the rating range is between 0.5 and 5.
Next, we check how many ratings a movie normally gets to see if the movies are getting enough number of ratings to have a rating with certainty or less error.
```{r movie effect with number of rating, echo=FALSE,message=FALSE}
#exploring the data -- movie effect with number of rating
edx%>%group_by(movieId)%>%summarise(n=n())%>%
  ggplot(aes(n))+geom_histogram(bins = 30,color="black")+
  xlab("Number of Ratings")+
  ggtitle("Histogram - Number of Ratings per Movie")
```
From the visualization we see, a large portion of movies has less than 100 ratings. Let’s look into more details:
```{r summary movie effect, echo=FALSE}
rating_number<-edx%>%group_by(movieId)%>%summarise(n=n())
summary(rating_number$n)
```
The median of number of ratings is only 122 while the maximum is 31362. These show that the number of ratings is an influence factor of the rating parameter per movie.
In the same way let us look at the number of ratings per user and if there is the same pattern. 
```{r plot number of rating per user, echo=FALSE}
edx%>%group_by(userId)%>%summarise(n=n())%>%
  ggplot(aes(n))+geom_histogram(bins=30,color="black")+
  xlab("Number of Ratings")+
  ggtitle("Histogram - Number of Ratings per User")
```
Most users give less than 100 ratings. It concludes that, the number of rating penalty term for users and movies are necessary to be introduced into the algorithms.

2.3.Insights and modelling approach

From the last chapter we see the model should include the effects from the mean rating, the movie and user effects with respectively the number of rating penalty.  

2.3.1.General strategy

We will first generate a train(90%) and test(10%) subset from edx. The models will be trained and tuned on these two subsets to get our final model. With this final model, we make prediction on validation dataset and get the final RMSE value.
```{r,echo=FALSE}
index<-createDataPartition(edx$rating,times=1,p=0.1,list=FALSE)
edxtrain<-edx[-index,]
tmp<-edx[index,]

edxtest <- tmp %>% 
  semi_join(edxtrain, by = "movieId") %>%
  semi_join(edxtrain, by = "userId")
```


In general, we can see a rating is the sum of average rating, movie effects, user effects, genres effect and residual error. Following is the equation of predicated rating Yu,I  with the first model:
$$ Y_{u,i}= \mu+b_{u}+b_i+b_g+\varepsilon_{u,i}  $$
With:	λ: the penalty,

bi: the movie effect,

bu: the user effect,

bg: the genres effect.

With this model, we can check once if the RMSE reaches the targeted value. In case not, we will need to introduce the number of rating penalty λ for the two effects, as they would have bigger errors, if the number of ratings was too less. The equation will be changed to the following:

$$Y_{u,i}=\mu+\frac{1}{n_i+\lambda}\sum_{n=0}^nb_i+\frac{1}{n_u+\lambda}\sum_{n=0}^nb_u+\varepsilon_{u,i}$$
With:	λ: the penalty,

ni: the number of rating of each movie,

nu: the number of rating of each user,

ng: the number of rating of each genre.


For this second model, different λ values can be used to make the prediction with the test dataset and calculate the RMSEs. Here we make the RMSE equation to a function in R:
```{r RMSE function, echo=TRUE}
RMSE <- function(true_ratings, pred){ 
  sqrt(mean((true_ratings - pred)^2))
}
```
In next chapter, we explain in detail how to calculate the parameters.

2.3.2.Parameters in the model

We have the value of µ already calculated from the last chapter. Let us further calculate the bi. It is generally the average rating of a movie and extracted µ out of it. Then we visualize it in a box plot to see the distribution:
$$b_i=1/n_i\sum_{n=0}^n(y_i-\mu)$$


```{r bi,message=FALSE}
#modeling -- movie effect parameter bi
bi<-edxtrain%>%group_by(movieId)%>%summarise(bi=mean(rating-mu))
```
```{r plot bi, echo=FALSE}
bi%>%ggplot(aes(bi))+geom_boxplot()+ggtitle("Boxplot of bi")
```

The next parameter to calculate is bu. The formula and box plot are the following:
$$b_u=1/n_u\sum_{n=0}^n(y_i-\mu-b_i)$$
```{r bu, message=FALSE}
bu<-edxtrain%>%left_join(bi,by="movieId")%>%
   group_by(userId)%>%summarise(bu=mean(rating-bi-mu))
```
```{r plot bu, message=FALSE, echo=FALSE}
bu%>%ggplot(aes(bu))+geom_boxplot()+ggtitle("Boxplot of bu")
bg<-edxtrain%>%left_join(bi,by="movieId")%>%left_join(bu,by="userId")%>%group_by(genres)%>%
  summarise(bg=mean(rating-bi-bu-mu))

```

The same way to calculate bg. For the more complex model with λ we give new names for the regularized two effects: bil,bul,bgl
$$bil=\frac{1}{n_i+\lambda}\sum_{n=0}^nb_i$$
$$bul=\frac{1}{n_u+\lambda}\sum_{n=0}^nb_u$$
$$bgl=\frac{1}{n_g+\lambda}\sum_{n=0}^nb_g$$
In the following code, l represents λ.


bil<-edxtrain%>%
     group_by(movieId)%>%summarise(bil=sum(rating-mu)/(n()+l))

bul<-edxtrain%>%left_join(bil,by="movieId")%>%
     group_by(userId)%>%summarise(bul=sum(rating-bil-mu)/(n()+l))
     
bgl<-edx%>%
  left_join(bil,by="movieId")%>%left_join(bul,by="userId")%>%group_by(genres)%>%
  summarise(bgl=sum(rating-bil-bul-mu)/(n()+l))


λ can be any values, however there will be a best value, which has the lowest RMSE, that we finally use. We use the RMSE from the best λ as the result of the complex model. Now we start to make the prediction with the two models.



3.Results

Let us make the prediction and check the RMSE with the first model, to see if the goal of the project is already reached. After joining the columns bi and bu to edx, the predicted rating is calculated by the following formula. Then we apply the RMSE function to evaluate the prediction:
$$ Y_{u,i}= \mu+b_{u}+b_i+b_g  $$
```{r,message=FALSE,echo=TRUE}
#prediction of the first model
pred1<-edxtest%>%left_join(bi,by="movieId")%>%left_join(bg,by="genres")%>%
  left_join(bu,by="userId")%>%mutate(pred=mu+bi+bu+bg)%>%pull(pred)
## calculating the RMSE of the prediction
rmse1<-RMSE(pred1,edxtest$rating)

rmse1
```
This result is better than just to predict using the mean value but not yet reaches the goal. Let us use the model with penalty λ. Here we use the sapply to calculate RMSEs using different λs and find out the lowest RMSE and respectively the λ:
$$ Y_{u,i}= \mu+bul+bil+bgl  $$
```{r sapply, message=FALSE, echo=TRUE}
lmda<-seq(1,10,0.25)
rmses<-sapply(lmda,function(l){
  mu<-mean(edxtrain$rating)
  
  bil<-edxtrain%>%
    group_by(movieId)%>%summarise(bil=sum(rating-mu)/(n()+l))
  
  bul<-edxtrain%>%left_join(bil,by="movieId")%>%
    group_by(userId)%>%summarise(bul=sum(rating-bil-mu)/(n()+l))
  
  bgl<-edxtrain%>%left_join(bi,by="movieId")%>%left_join(bu,by="userId")%>%group_by(genres)%>%
    summarise(bgl=sum(rating-bi-bu-mu)/(n()+l))
  
  pred2<-edxtest%>%
    left_join(bil,by="movieId")%>%left_join(bul,by="userId")%>%left_join(bgl,by="genres")%>%
    mutate(pred=mu+bil+bul+bgl)%>%pull(pred)
  return(RMSE(pred2,edxtest$rating))
})
```
From the following plot we find out the best λ= 4.5 with the lowest RMSE:
```{r, message=FALSE, echo=TRUE}
qplot(lmda,rmses)
best_lamda<-lmda[which.min(rmses)]
min(rmses)
```
This RMSE 0.8654673 is slightly bigger than 0.86490 but better than without regularization. Furthermore, when we use the entire edx dataset to train and test on validation, it is possible to get a better result. Therefore, we could already stop here and construct out final model.
Before we come to the conclusion of the final model. Let us have a look at the parameters before and after introducing the best λ, to see why we get a better result with the regularization:
```{r, message=FALSE, echo=FALSE}
bil<-edxtrain%>%
  group_by(movieId)%>%summarise(bil=sum(rating-mu)/(n()+4.5))
bul<-edxtrain%>%left_join(bil,by="movieId")%>%
  group_by(userId)%>%summarise(bul=sum(rating-bil-mu)/(n()+4.5))
##boxplot the bi and bil for compare
bi_value<-bi$bi
bil_value<-bil$bil
b1<-data.frame(bi_value,bil_value)
par(mgp=c(3,2,0))
boxplot(b1,main="Compare of bi value")
```
We can see the bil value has smaller variety but has the similar median value compare to bi. Our final model can be written in the following way:
$$Y_{u,i}=\mu+\frac{1}{n_i+4.5}\sum_{n=0}^nb_i+\frac{1}{n_u+4.5}\sum_{n=0}^nb_u+\frac{1}{n_g+4.5}\sum_{n=0}^nb_g$$

Now the very last step is to calculate RMSE on the validation:
```{r final RMSE, message=FALSE, echo=TRUE}
mu<-mean(edx$rating)
bil<-edx%>%
  group_by(movieId)%>%summarise(bil=sum(rating-mu)/(n()+4.5))
bul<-edx%>%left_join(bil,by="movieId")%>%
  group_by(userId)%>%summarise(bul=sum(rating-bil-mu)/(n()+4.5))
bgl<-edx%>%
  left_join(bil,by="movieId")%>%left_join(bul,by="userId")%>%group_by(genres)%>%
  summarise(bgl=sum(rating-bil-bul-mu)/(n()+4.5))
pred3<-validation%>%
  left_join(bil,by="movieId")%>%left_join(bul,by="userId")%>%left_join(bgl,by="genres")%>%
  mutate(pred=mu+bil+bul+bgl)%>%pull(pred)
RMSE(pred3,validation$rating)

```
With the final model we get the RMSE value smaller than the 0.86490. Therefore, we reached the goal of our project.


4.Conclusion

In this chapter we will summarise the project and inspect the limitation from different aspects and explain how to improve the future work.
In this project, we got the required dataset and were given the task of building up a prediction model with a goal RMSE value. The first and important thing to do is always cleaning the data and explore the data to get the insights. After that we built up different equations of models based on the patterns we found out from last step. In the end we tested the different models and chose the best out it. We tested the RMSE values from the final model. The targeted goal value is reached. There can be some improvement in the future work.
The limitation during project is, two processes take longer operation time and even sometimes crash the computer due to limitation of the RAM:  data cleaning with str_ function and the RMSE calculation using different λ with sapply function. Additional, in the preparation of the project, the cross validation on the edx to train a model was also tested, but the Rstudio was crashed, therefore, the sub train and test set was used to construct the model.
Furthermore, the algorithms can be further improved to reach a lower RMSE. In this model, the movie general rating effect, user effect and genres effect with their number of rating penalty are introduced into the algorithms. With those parameters, the RMSE value does reach the target of the project. However, the year effect can also be considered into the algorithms in case of reaching lower RMSE value. Addition to that, the cross validation method could be researched and compare to the data participation method which is used in this project.

 
